{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import spacy \n",
    "import nltk\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import tensorflow as tf \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#To Temporarily Remove Deprecation Warning\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ad0ad08e938b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/combined_news.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/combined_news.csv')\n",
    "data.dropna(inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['length'] = data['text'].apply(lambda x: len(x.split(' ')))\n",
    "sns.distplot(data['length'])\n",
    "data['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main tokenizing function\n",
    "def tokenize_line(line, chars_to_exclude, stopwords, tokenizer, stem_tokens=False, stemmer=None, lemm_tokens=False, lemmatizer=None):\n",
    "    # removing unwanted characters and numbers from the string\n",
    "    pattern = '[' + '|'.join(list(chars_to_exclude)) + '|\\d]*'\n",
    "    line = re.sub(pattern, \"\", line).lower()\n",
    "\n",
    "    # generating tokens\n",
    "    tokens = [token for token in tokenizer.tokenize(line) if token not in stopwords]\n",
    "    \n",
    "    # stemming the tokens if the user wants to\n",
    "    if stem_tokens:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # lemmatizing the tokens if the user wants to\n",
    "    if lemm_tokens:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to help with tokenizing columns\n",
    "def tokenize_lines(lines, chars_to_exclude, stopwords, tokenizer=None, stem_tokens=False, stemmer=None, lemm_tokens=False, lemmatizer=None):\n",
    "\n",
    "    all_tokens = [tokenize_line(line, chars_to_exclude, stopwords, tokenizer, stem_tokens, stemmer, lemm_tokens, lemmatizer) for line in lines]\n",
    "\n",
    "    return np.array(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the default parameters for now\n",
    "def get_default_tokenization_params():\n",
    "    chars_to_exclude = string.punctuation\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "\n",
    "    #Modify these according to the user's choices\n",
    "    stem_tokens = False\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    lemm_tokens = True\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    return (chars_to_exclude, english_stopwords, tokenizer, stem_tokens, stemmer, lemm_tokens, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#To-Do: Brush up the following code to highlight the top topics discussed in the data sets: \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "X = tokenize_lines(data['text'], *get_default_tokenization_params())\n",
    "Y = data['label']\n",
    "data_processed = pd.DataFrame(({'title': data['title'], 'text': X, 'label': Y}))\n",
    "\n",
    "\n",
    "# Subsequent section attempts to perform some Topic Modeling using LDA \n",
    "\n",
    "# This section focuses on real news\n",
    "real_news = data_processed[data_processed['label'] == 'Real']\n",
    "num_topics = 10 \n",
    "num_features = 100\n",
    "'''\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=num_features, stop_words='english')\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, max_iter=5, learning_method='online', learning_offset=50., random_state=0)\n",
    "lda_pipeline = Pipeline([('vectorizer', vectorizer), ('lda', lda)])\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "data_vectorized = vectorizer.fit_transform(data_processed['text'])\n",
    "dash = pyLDAvis.sklearn.prepare(lda_pipeline.steps[1][1], data_vectorized, vectorizer, mds='tsne')\n",
    "pyLDAvis.save_html(dash, 'real_news_lda.html')\n",
    "\n",
    "# Repeat the above for fake news\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtext= real_news['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gensim library for additional topic modeling and statistical anlysis\n",
    "#corpora module implements dictionary-mapping btwn words and corresponding int IDs\n",
    "from gensim import corpora\n",
    "\n",
    "real_gensim_dictionary = corpora.Dictionary(realtext)\n",
    "#creating bag of words (bow) w/ realtext\n",
    "real_gensim_corpus = [real_gensim_dictionary.doc2bow(token, allow_update=True) for token in realtext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#saving dictionary and bow (via pickle )to use for predictions below\n",
    "filename1 = 'real_gensim_corpus.pkl'\n",
    "pickle.dump(real_gensim_corpus, open(filename1, 'wb'))\n",
    "real_gensim_dictionary.save('real_gensim_dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#creating LDA Model\n",
    "#takes v long to load\n",
    "real_lda_model = gensim.models.ldamodel.LdaModel(real_gensim_corpus, num_topics=num_topics, id2word=real_gensim_dictionary, passes=10)\n",
    "real_lda_model.save('real_gensim_model.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing 5 words per topic\n",
    "topics = real_lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating LDA: Topic Coherence\n",
    "#Perplexitiy: -8.169369315206076 (low)\n",
    "#Coherence: 0.4601994449293338 (high)\n",
    "#Perplexity is low and coherence is high which is expected as this is real news(?)\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_score_lda = CoherenceModel(model=real_lda_model, texts=realtext, dictionary=real_gensim_dictionary, coherence='c_v')\n",
    "coherence_score = coherence_score_lda.get_coherence()\n",
    "\n",
    "print('\\nPerplexity:', real_lda_model.log_perplexity(real_gensim_corpus))\n",
    "print('\\nCoherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic Modeling Visualization\n",
    "\n",
    "real_gensim_dictionary = gensim.corpora.Dictionary.load('real_gensim_dictionary.gensim')\n",
    "real_gensim_corpus = pickle.load(open(filename1, 'rb'))\n",
    "real_lda_model = gensim.models.ldamodel.LdaModel.load('real_gensim_model.gensim')\n",
    "\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "lda_visualization = pyLDAvis.gensim_models.prepare(real_lda_model, real_gensim_corpus, real_gensim_dictionary, sort_topics=False)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(lda_visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above but for fake news\n",
    "fake_news = data_processed[data_processed['label'] == 'Fake']\n",
    "faketext = fake_news['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_gensim_dictionary = corpora.Dictionary(faketext)\n",
    "#creating bag of words (bow) w/ realtext\n",
    "fake_gensim_corpus = [fake_gensim_dictionary.doc2bow(token, allow_update=True) for token in faketext]\n",
    "\n",
    "#pickling useful for efficiently storing data to be used later\n",
    "filename2 = 'fake_gensim_corpus.pkl'\n",
    "pickle.dump(fake_gensim_corpus, open(filename2, 'wb'))\n",
    "fake_gensim_dictionary.save('fake_gensim_dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_lda_model = gensim.models.ldamodel.LdaModel(fake_gensim_corpus, num_topics=num_topics, id2word=fake_gensim_dictionary, passes=10)\n",
    "fake_lda_model.save('fake_gensim_model.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"topics = fake_lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_score_lda = CoherenceModel(model=fake_lda_model, texts= faketext, dictionary=fake_gensim_dictionary, coherence='c_v')\n",
    "coherence_score = coherence_score_lda.get_coherence()\n",
    "\n",
    "print('\\nPerplexity:', fake_lda_model.log_perplexity(fake_gensim_corpus))\n",
    "print('\\nCoherence Score:', coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"fake_gensim_dictionary = gensim.corpora.Dictionary.load('fake_gensim_dictionary.gensim')\n",
    "fake_gensim_corpus = pickle.load(open(filename2, 'rb'))\n",
    "fake_lda_model = gensim.models.ldamodel.LdaModel.load('fake_gensim_model.gensim')\n",
    "\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "lda_visualization = pyLDAvis.gensim_models.prepare(fake_lda_model, fake_gensim_corpus, fake_gensim_dictionary, sort_topics=False)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(lda_visualization)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defining and Training the Model: \n",
    "There will be several different types of sequential operations and layers: \n",
    "\n",
    "1. A tokenizer to transform each article into a vector of tokens\n",
    "2. A word embedding layer that learns an embedding vector. \n",
    "3. A 1D convolutional and max-pooling layer -- this is to calculate the largest value in each feature map\n",
    "4. LSTM (Long Short-Term Memory) units: this will form the recurrent part ofn the recurrent convolutional neural network. \n",
    "'''\n",
    "\n",
    "# First step is to import all of the necessary libraries for this experiment\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout\n",
    "# Checking tensorflow version\n",
    "if float(tf.__version__[0]) < 2.0:\n",
    "    print(\"Updating Tensorflow\")\n",
    "    !pip install --upgrade tensorflow\n",
    "else: \n",
    "    print(\"Correct Version of Tensorflow installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd010204038df05068d31bf1ae3a20e37075375f2e1f68f8a0c7c018d5491aaf462",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "10204038df05068d31bf1ae3a20e37075375f2e1f68f8a0c7c018d5491aaf462"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}