{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defining and Training the Model: \n",
    "There will be several different types of sequential operations and layers: \n",
    "\n",
    "1. A tokenizer to transform each article into a vector of tokens\n",
    "2. A word embedding layer that learns an embedding vector. \n",
    "3. A 1D convolutional and max-pooling layer -- this is to calculate the largest value in each feature map\n",
    "4. LSTM (Long Short-Term Memory) units: this will form the recurrent part ofn the recurrent convolutional neural network. \n",
    "'''\n",
    "\n",
    "# First step is to import all of the necessary libraries for this experiment\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/raid/tft-data/data/combined_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting labels from strings to ints\n",
    "data[\"label\"].replace({\"Real\": 1, \"Fake\": 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing Data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size=len(word_index)\n",
    "\n",
    "#Padding data\n",
    "#Keras prefers inputs to be vectorized and all inputs to have the same length. \n",
    "#We will pad all input sequences to have the lengthWe will pad all input sequences to have specific length\n",
    "sequences = tokenizer.texts_to_sequences(data['text'])\n",
    "padded = pad_sequences(sequences, maxlen=500, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compartmentalizing data into training set and testing set\n",
    "split = 0.2\n",
    "split_n = int(round(len(padded)*(1-split),0))\n",
    "\n",
    "train_data = padded[:split_n]\n",
    "train_labels = data['label'].values[:split_n]\n",
    "test_data = padded[split_n:]\n",
    "test_labels = data['label'].values[split_n:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pre-Trained GloVe for Keras embedding layer\n",
    "embeddings_index = {};\n",
    "with open('/raid/tft-data/Glove/glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split();\n",
    "        word = values[0];\n",
    "        coefs = np.asarray(values[1:], dtype='float32');\n",
    "        embeddings_index[word] = coefs;\n",
    "print(len(coefs))\n",
    "\n",
    "embeddings_matrix = np.zeros((vocab_size+1, 100));\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word);\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size+1, 100, weights=[embeddings_matrix], trainable=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=4),\n",
    "    tf.keras.layers.LSTM(20, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(20),\n",
    "    tf.keras.layers.Dropout(0.2),  \n",
    "    tf.keras.layers.Dense(512),\n",
    "    tf.keras.layers.Dropout(0.3),  \n",
    "    tf.keras.layers.Dense(256),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training model\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=2, batch_size=100, validation_data=[test_data, test_labels])\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the Results\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
