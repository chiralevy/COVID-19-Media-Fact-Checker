*Project Members: Chira Levy, Hussein Faara, Jorge Rodriguez*

## Project Updates: 
Keep up to date with our work through our [Project Updates page](project-updates.md)! 

## Introduction
With the advent of social media, many barriers required to publish information are effectively removed, granting many individuals and organizations the ability to rapidly disseminate information to reach mass audiences. It is no surprise,then,that this enhanced ability can be used to spread true and false information, and has become an important concern, fueling a growing demand for the fact-checking of online content.

In journalism, fact-checking is the task of assessing the validity, or veracity, of a claim. It is a task that is usually performed manually by trained fact-checkers: typically the procedure entails verifying whether a piece of text or a claim is accurate by utilizing relevant information from various trustworthy sources to infer whether a claim is factually supported or not. This procedurecan be very time consuming depending on a couple of parameters: for example, the complexity of the claim and the evidence needed to provide a verdict. With online news outlets on the rise and an increasingly engaged global social media base,  there is a growing need to fact-check and detect potential loci of misinformation (e.g. fake news) found in information providers across the internet in a more  efficient  manner. In fact, in 2020  alone, fake news websites significantly increased their share of engagement on social media platforms making up nearly one fifth (17%) ofall likes, shares, comments – a stark contrast to roughly 8% in 2019. These figures bear significant importance as interactions with fake news and fake claims on social media can potentially exacerbate political polarization, quickly skew opinions, and negatively influence how people perceive crises – for  example, the ongoing SARS-CoV-2 pandemic. This raises the question: what, then, can be done to help journalists and trained fact-checkers address and mitigate the fake news and misinformation problem? Enter fact-checking and classification. 

Fact-Checking can be traced back to 2011, when scholars Cohen et al. suggested it as one of the tasks that should (at the very least) be aided by machines, or automated since the relevant technology begins to suggest the possibility. Later in 2014 the task is not only introduced by Vlachos and Riedel but they also compile a data set and propose an end-to-end algorithm suitablefor completing the task. Subsequent research over the past 6 years has demonstrated the popularityof the task. This rise in popularity is partly due to the progress made in relevant fields like Natural Language Processing, information retrieval, and increased access to robust data sets/bases.

As part of the ongoing discourse in the field, this project will attempt to provide a comprehensive overview of the state of the field, as well as address what is a significant lack of oversight in quality filters of social media companies by taking a more deliberate approach in classifying what is misinformation and what is not. Using a neural network classifier trained on recent datasets, our goal is to flag articles that either intentionally or unintentionally include

```diff
Should we end this sentence with: 
- "untrue statements about COVID or COVID-related topics."
- "misinformation." 
```
We aim for at least 85% accuracy and specifically less than 7.5% of false negatives (news classified as trustworthy when not). If time allows, we hope to use text summarization to create digestible versions of trustworthy sources (END HERE OR INCLUDE: "that include pertinent information regarding COVID, but require specialized knowledge in the field"). A technical challenge we will face is the reliance of the data used to train our classifier. Most, if not all, of the state-of-the-art fake news detection systems rely on data that could easily be converted into a vector and fed to a model.

## What is Fact-Checking? 
From a researcher’s point of view, Vlachos and Riedel (2014) define fact-checking as “the assignment of a truth value to a claim made in a particular context.” Traditionally the task hasbeen commonly performed by trained professionals. In fact, a trained fact-checker must consolidate previous publications or known  facts, combined with reasoning, to reach a verdict on an articlebefore publishing it. Due to the large amount of content produced and shared each second on onlineoutlets and social media nowadays, false information spreads at an unprecedented speed. The time-consuming traditional method of fact-checking is clearly insufficient. Fortunately,rapid progress in the fields of natural language processing, databases, and information retrievalover the past decade have made delegating the task to machines more plausible. Therefore, apartfrom attempting to accurately assess the truthfulness of claims, automated fact-checking attemptsto reduce the human burden of assessing such claims.

Although what an automated fact-checking system should do is intuitive, several scholars haveframed it in different ways.  For example, Vlachos and Riedel highlight that it is not necessarily abinary classification task because many statements are neither completely true nor completely false.As a result, some fact-checking data sets label the claims with varying veracity levels. Accordingto Hassan et al., an ideal automated fact-checking system should not only be able to make accurateassessments, but should also be fully automated, instant, and accountable. This is very challengingtask because it requires solving the often  difficult computational problems of natural language,understanding context, retrieving relevant information, and reasoning. Furthermore, an ability toexplain its decision with supporting evidence from trusted sources is generally expected of any good automated fact-checking system, complicating the task even further.

## Methods:
  There is a tendency in people to conceive what they read from news sources and/or social media sites to be completely true -- even if the news source admits to their mistakes retroactively. It is important to identify fake news from the real news and/or check whether claims made are valid or not -- especially during our protracted times with the SARS-COV-2 virus. This problem can be tackled with the help of Natural Language Processing tools which will aid in identifying fake or reliable news based on historical data. The following is an outline of how we plan on constructing our classifier and what data sets it will be pulling from in the training process. 
  
  We constructed our classifier with the help of libraries like Keras and Tensorflow, to ensure proper understanding of its function and to ease modifications when deemed necessary. The classifier is a recurrent convolutional neural network model that consists of several different types of sequential operations and layers:
1. We will utilize a tokenizer to transform each article into a vector of indexed tokens (1 token = 1 word).
2. Word Embedding Layers 
3. 1D convolutional and max-pooling layers.
4. Long Short-Term Memory layers.

### Datasets:
The following datasets will prove useful in our experiments:
1. [Fake News Data](https://www.kaggle.com/c/fake-news/data)
2. [COVID-19 Fake News Dataset 1](https://www.kaggle.com/arashnic/covid19-fake-news)
3. [COVID-19 Fake News Dataset 2](https://www.kaggle.com/thesumitbanik/covid-fake-news-dataset)

Our classifier was trained on a general data set (the first linked data set above) of news articles that fall under two labels: Real or Fake. We will do some preliminary analysis using the LDA module from Scikit-learn and the pyLDAvis library to compare topics and most significant terms in real and fake news articles. Our hope is to create an interactive visualization of the aforementioned details for both real and fake news. Subsequent sections will take a closer look at the results. 

## Discussion: 
The bulk of our project attends to the datasets 1 and 2 enumerated in our "Datasets" section. To better understand our data set, we think it is important to consider the distribution of fake news versus real news, length of articles, and even the topic breakdown of the articles sourced from the data sets we selected for this project. Other metrics that allow us to understand distinguishing features between real and fake news is Perplexity and Coherence. 

![Image](https://github.com/chiralevy/cs152sp21-project/blob/gh-pages/pictures/distribution.png) ![Image](https://github.com/chiralevy/cs152sp21-project/blob/gh-pages/pictures/meta-data.png) ![Image](https://github.com/chiralevy/cs152sp21-project/blob/gh-pages/pictures/article%20lengths.png)

Our data set was composed of a total of 44,898 articles with meta-data such as the title, topic, publishing date, and corresponding label included. Upon further inspection of the articles, we constructed a topic model using Latent Dirichlet Allocation (LDA). Interestingly, we found that (INSERT FINDINGS HERE)

Our model achieved an overall accuracy of (INSERT ACCURACY OF MODEL HERE). While accuracy is a useful metric to assess the performance of a classifier, it fails to tell us how the model is performing relative to each class. In order to have a robust understanding of our classifier's performance, we created the following confusion matrix to understand the distribution of true positives, false positives, false negatives, and true negatives. 
(INSERT CONFUSION MATRIX HERE)

In addition, we hope to compose a classification report to provide the Precision, Recall, and F-score of our model. Now, the question is raised: how does our approach fair with other documented strategies? Let's find out! 

### Other Data Sets: 
Though a major bottleneck associated with automated fact-checking is the limited number of data sets, they are an indispensable component to the growth of fact-checking research as most fact- checking systems rely on machine learning algorithms.

During the task’s inception, data sets for fact checking demonstrated positive signs towards a rigorous formulation of the fact-checking task. However, due to their small size, none were sufficient for training machine learning models. Since then, a number of data sets have been more fully constructed, deeming them more viable to train classifiers.

One such data set is Wang’s Liar Liar, which compiles 12.8k human-labeled short statements from PolitiFact. Truthfulness ratings are assesses at 6 levels: pants-fire, false, barely-true, half- true, mostly-true, and true – and the classes are fairly balanced. The statements are sourced from various contexts, venues, topics, and speakers with meta-data on their affiliations, job, state, and even credit history. As a result of Wang’s experiments, it is suggested that the inclusion of meta- data in the input improves the accuracy slightly. The flaw of Liar, as Alhindi et al. points out, is that it does not include the evidence used by humans to judge a claim, limiting the capabilities of data-driven approaches that train on it.

Another data set is Thorne et al.’s Fact Extraction and VERification (FEVER) data set. It consists of 185,445 claims, each of which is labeled as ”Supported”, ”Refuted”, or ”NotEnoughInfo”. Included is the evidence that supports or refutes the claim – only in the case of the first two classes. The claims were extracted from Wikipedia, then altered and labelled by human annotators. The evidence sentences are collected during the labelling process. Along with the data set release, the authors hosted the Fact Extraction and VERification (FEVER) Shared Task, which requires systems to provide not only the correct label but also the correct piece of supporting or refuting evidence. In 2018, the winning team achieved a score that is only about 6% lower than the state- of-art as of 2019, and the FEVER data set has since been widely used to evaluate fact-checking systems in the past two years.

Another data set is presented by Augenstein et al.. Their data set consists of 34,918 claims from 26 English fact-checking websites. Each claim is accompanied by rich metadata and 10 evidence pages retrieved from Google search. Within each claim, entities such as people and places are also detected, disambiguated, and linked to their corresponding Wikipedia page.

### Other Fact-Checking Methods: 
1. Claim Buster
ClaimBuster is an ongoing project toward automated fact-checking that determines check-worthy claims to assist human fact-checkers Hassan et al. [2017]. ClaimBuster models the fact-checking task as a supervised 3-class classification problem, where outputs are used to rank which sentences are check-worthy. 30 features are selected with a random forest classifier from a pool of features: sentiment, length, word, and POS tag, and Entity Type. The classification task achieves 74% and the ranking portion achieves 96% accuracy with Support Vector Machine(SVM). The creator used it on the 2015 GOP debate and had a good overlap with sentences checked by PolitiFact, CNN, and FactCheck.org. Hassan et al. tests whether subjectivity analysis would be able to discern between non-factual sentences, defined as subjective sentences (e.g. opinions, beliefs), but found no improvements in experimental results. Furthermore, they provide the current status of ClaimBuster which cosists of the following components: Claim Monitor continuously monitors and retrieves texts from a variety of different sources; Claim Matcher gives an important factual claim within a repository of fact-checked claims; Claim Checker collects supporting or debunking evidence from knowledge bases and the web given a claim; And Fact Check Reporter synthesizes a report by combining the aforementioned evidence and delivers it to users. Overall, ClaimBuster presents a number of tools that are of great interest to automated fact-checking and the overall domain of computational journalism.

2. FakeBox
As of 2019, state-of-art fake news detector, Fakebox, does without evidence seeking. Instead it analyzes linguistic characteristics of news articles from the headline to the content and inspects the site’s domain to evaluate a score. It reportedly achieves classification accuracy upwards of 95%. In their paper, Zhou et al.demonstrate how tampered real news articles can evade detection due to its stylistic resemblance to a real article. As a result, they propose evidence-based fact checking as a potential solution to address the vulnerability. Another noteworthy contribution from the article is that fact-checking is not included in any current fake news detection models as of then.

In contrast to the above, Rashkin et al. (2017) label their work as political fact-checking, not fake news detection. They divide their examination into two parts: linguistic analysis of fake news and a truthfulness prediction model for political fact-checking. Of interest is their analysis of linguistic features. For example, first-person, second-person pronouns and exaggerating words appear unusually more frequently in deceptive news. As for the prediction task, they experiment with Naive Bayes, Maximum Entropy, and LSTM (Long Short-Term Memory) using sequences of words as input. They also tried adding LIWC (Linguistic Inquiry and Word Count) features before the activation layer, but it shows little effect on the performance. The maximum top-3 ranking test accuracy for a 6-class classification was 22%, compared to 6% by the majority baseline.

Despite achieving high accuracy, Fakebox’s sole reliance on linguistic characteristics and meta- data presents two major flaws. The first of which is the system’s fragility against misinformation and tampered (or artificial) fake news, which is stylistically identical to real news; And the second being its inability to provide supporting evidence to substantiate its verdict.

3. Evidence Awareness with FEVER
Alhindi et al. (2018) extends Liar by extracting human justifications from fact-checking articles associated with the claim, with all verdict-identifying words removed in order to prove that the lack of evidence used by humans to judge a claim limits the capabilities of data-driven approaches that train on it. In fact, their experimental results show that additional contextual evidence improves the fact-checking prediction accuracy, pushing the significance of incorporating evidence even further.

Because purely linguistic approaches have limitations, and because the ability to provide facts to support their decisions is beneficial, most of publishable fact-checking systems rely on evidence and are outlined as a multi-stage task with 3-4 subtasks. One of the earliest full-pipeline attempts at fact-checking was Thorne et al.’s proposed baseline for the FEVER data set. Their system is comprised of three phases, all of which utilize existing NLP techniques and models. The three phases are: document retrieval, sentence selection, and textual entailment recognition – which they define as the task of predicting whether the facts in the first sentence imply those in the second. The baseline algorithm achieves 31.87% accuracy for correct class and correct evidence, which is lower than 50.91% if a requirement for evidence is omitted.

Showing more promise is Yoneda et al. (2018)’s system which achieves a FEVER score of 62.52 – they also placed second at the first FEVER Shared Task! Their system compromises four phases: document retrieval, sentence retrieval, natural language inference, and aggregation-reranking. Error analysis suggests that the frequent failure cases are caused by similar word embeddings – in the context of NLP, it refers to the numerical vector representation of words – of unique information and complex sentences that require relationships between multiple words which is not captured by the alignment. More precise word embeddings that capture numerical features or are trained for the specific context are some viable solutions.

In the same year, Nie et al. present a system that placed first at the same FEVER Shared Task. In contrast to Yoneda et al.’s, Nie et al.’s system consists of three phases: document retrieval, sentence selection, and claim verification – fact extraction and verification. Noteworthy is that all three phases utilize a homogeneous 4-layer deep neural semantic matching network. the model does not utilize existing Natural Language Inference (NLI) models to obtain relationship between claims. Instead, in the third phase, the model is trained from scratch through a concatenation of word embeddings, normalized semantic relatedness scores from the initial two phases, ontological WordNet features, and additional embeddings to uniquely encode numbers. It achieves 64.23 FEVER score and was the state-of-art at the time.

More recently, Soleimani et al. (2019) proposed a fact-checking system that utilize Bidirectional Encoder Representations from Transformers (BERT), one of the best pre-trained language models to date. The system consists of two BERT models: one for retrieving relevant evidence sentences and another for claim verification. In developing the models, the authors experiment with point- wise and pairwise approaches in finetuning BERT, and both outperform most existing systems in recall. In addition, they also examine the effect of Hard Negative Mining (HNM)in the evidence retrieval BERT model and found that it slightly improves the system’s performance. Through their experiments on the FEVER data set, the pipeline achieves an almost-state-of-art FEVER score of 69.66.

4. DREAM
Zhong et al. presents the state-of-art fact checking model, abbreviated as ”DREAM”, which achieves 70.60 FEVER score and 76.85% label accuracy. It shares the same 3-stage system com- ponents as the baseline proposed in Thorne et al. (2018). Interestingly, the document retrieval component mostly follows Nie et al.’s approach, and the sentence selection component is framed as a semantic matching problem extending from pre-trained language models. What’s unique about their approach is the claim verification component: it uses semantic structures of the evidence in- stead of simple string or feature concatenation. In doing so, it adopts a graph structure to capture relative distances between words allowing it to measure how semantically related they are. There- after, it takes advantage of graph convolutional and graph attention networks to propagate and aggregate information on the graph. Despite the added layers, the model is not without any faults. In the error analysis, Zhong et al. list that the frequent sources of errors are failures to match different phrases that refer to the same thing in the particular context, and misleading evidences retrieved from the earlier stages.

## Moving Forward:
1. A Twin Challenge
Recently, Nakov et al. (2021) outline two challenges towards automated fact-checking: first, to develop practical tools to solve the problems trained fact-checkers face; second, to demonstrate the value of their tools to trained fact-checkers in their daily work. Fact-checking is not a straightforward or routine process. While it requires a series of steps that attend to sensing media, determining check-worthy claims, and ending with assessing the truthfulness of a claim, there seems to be a lack of collaboration between researchers in collaboratively developing automated systems. While Nakov et al. do share a list of tasks performed by current fact-checking systems that can sup- port trained fact-checkers in their craft, they also suggest that there is little overlap between what trained fact-checkers want and what the scholarly community considers a priority.

2. Technological Limitations
Like most machine learning and deep learning models, most state-of-art fake news detection systems rely on data that could be easily converted into a vector and fed to the model such as metadata and word embeddings. However, this approach is vulnerable to certain types of fake news, such as machine-generated fake news, as demonstrated by Schuster et al. and Zhou et al.. Because fact- checking attends to the truthfulness of a claim, sourcing relevant information from a repository is unavoidable. As is the case with the three-stage systems used by Thorne et al. and Nie et al., the system begins with document retrieval and end with fact verification, relying on very different techniques from various areas of computer science. Of interest is evidence retrieval which appears to be most problematic. This is evident in DREAM which achieves some of the best performance to date and help identify poor performance in the evidence retrieval component(s) as a source of inaccuracies. Also noteworthy is that FEVER, the data set used to evaluate DREAM, limits the justification for its assessment to sentences from Wikipedia. In doing so, the FEVER data set controls the number of sources to pull from, and implicitly assumes that all sentences from Wikipedia can be trusted. This is much unlike the rationale produced by human fact-checkers. Therefore, expanding the knowledge base and classifying reliable sources are two more areas that fact-checking systems need to address before deployment.


## Ethical Concerns
While the benefits of accurate fact-checking systems are obvious, there are also some practical concerns that deserve attention. Relevant is the question: What are the consequences of deploying automated fact-checking systems?
Important to understanding the possible consequences of deploying these systems is exploring how real-life people interact with such systems. Nguyen et al. explores this through their exam- ination of how users interact with AI fact-checking systems. Their system in the experiment is a transparent and interactive human-AI interface that helps users decide when to trust its decision. They observe that AI can be both helpful and misleading through their user study that asks par- ticipants to use the system to aid their personal assessment of a claim’s truthfulness. Particularly concerning is that the user’s accuracy improve on statements that are correctly classified and falls on statements that are incorrectly classified. What is clear is the following: On the one hand, fact- checking tools could indeed increase an individuals accuracy in deciding whether to trust a claim; on the other hand, it runs the risk of reducing an individual’s exercising of personal judgment should the tool makes systematic mistakes.

Another issue is raised by Oshikawa et al.. While one could argue that meta-data helps the accuracy in fact-checking and fake news detection, it is wise to consider the problematic nature of a fact-checking system’s assessment of a claim’s source – e.g. the person, news network, organization. This is even more clear in the case of determining whether something is truthful based on the claim’s author. For example, a system may learn to attach high trustworthiness to prominent political figures or reputable news sources; consequently, this could lead to the silencing of less prominent but reputable voices. In a similar venue, the question of who is accountable for a system’s errors is brought into the spotlight. Nguyen et al. and Oshikawa et al. address two different yet equally valid points. As is the case with any of machine learning tool, the creators of automated fact-checking systems are responsible for detecting and minimizing bias in their systems. However, users bear some responsibility in using these tools as aids to help (not replace) their ability to judge claims.




## Related Works
For a more comprehensive look at the resources that inform our work, please refer to our [literature review](literature-review.md).

1. Alhindi, T., Savvas Petridis, and Smaranda Muresan. Where is your evidence: Improving fact- checking by justification modeling. In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 85–90, 2018.
2. Augenstein, I., Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Christian Hansen, Jakob Grue Simonsen . MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4677–4691, nov 2019. URL http://arxiv.org/abs/1909.03242.
3. Baly, R., Mitra Mohtarami, James Glass, Lluis Marquez, Alessandro Moschitti, and Preslav Nakov. Integrating stance detection and fact checking in a unified corpus. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, pages 21–27, 2018.
4. Cohen, S., Chengkai Li, Jun Yang, and Cong Yu. Computational journalism: A call to arms to database researchers. In 5th Biennial Conference on Innovative Data Systems Research, CIDR, pages 148–151, 2011.
5. Ding, Lixuan, Lanting Ding, and Richard O. Sinnott. "Fake News Classification of Social Media Through Sentiment Analysis." International Conference on Big Data. Springer, Cham, 2020.
6. Hassan, N., Fatma Arslan, Chengkai Li, and Mark Tremayne. Toward automated fact- checking: Detecting check-worthy factual claims by claimbuster. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’17, page 1803–1812, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450348874. doi: 10.1145/3097983.3098131. URL https://doi.org/10.1145/3097983. 3098131.
7. Hamid, Abdullah, et al. "Fake News Detection in Social Media using Graph Neural Networks and NLP Techniques: A COVID-19 Use-case." arXiv preprint arXiv:2012.07517 (2020).
8. Kaliyar, Rohit Kumar, et al. "FNDNet–a deep convolutional neural network for fake news detection." Cognitive Systems Research 61 (2020): 32-44.
9. Le, Thai, Suhang Wang, and Dongwon Lee. "Malcom: Generating malicious comments to attack neural fake news detection models." arXiv preprint arXiv:2009.01048 (2020). URL: https://arxiv.org/pdf/2009.01048.pdf
10. Nakov, P., David Corney, Maram Hasanain, Firoj Alam, Tamer Elsayed, Alberto Barron- Cede, Paolo Papotti, Shaden Shaar, and Giovanni Da San Martino. Automated fact-checking for assisting human fact-checkers. In EURECOM, editor, Submitted to ArXiV, 13 March 2021, 2021.
11. Nguyen, A., Aditya Kharosekar, Saumyaa Krishnan, Siddhesh Krishnan, Elizabeth Tate, By- ron C. Wallace, and Matthew Lease. Believe it or not: Designing a human-ai partnership for mixed-initiative fact-checking. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology, UIST ’18, page 189–199, New York, NY, USA, 2018. Asso- ciation for Computing Machinery. ISBN 9781450359481. doi: 10.1145/3242587.3242666. URL https://doi.org/10.1145/3242587.3242666.
12. Nie, Y., Haonan Chen, and Mohit Bansal. Combining fact extraction and verification with neural semantic matching networks. CoRR, abs/1811.07039:6859–6866, 2018. URL http://arxiv.org/ abs/1811.07039.
13. Oshikawa, Ray, Jing Qian, William Yang Wang. A Survey on Natural Language Processing for Fake News Detection. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 6086–6093, nov 2018. URL http://arxiv.org/abs/1811.00770.
14. Rashkin, H., Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi. Truth of varying shades: Analyzing language in fake news and political fact-checking. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2931– 2937, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1317. URL https://www.aclweb.org/anthology/D17-1317.
15. Schuster, Tal, Darsh J Shah, Yun Jie Serene Yeo, Daniel Filizzola, Enrico Santus, Regina Barzilay. Towards Debiasing Fact Verification Models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3410–3416, 2019. ISBN 9781950737901. doi: 10.18653/v1/d19-1341. URL https://arxiv.org/pdf/1908.05267.pdf.
16. Soleimani, A., Christof Monz, and Marcel Worring. BERT for evidence retrieval and claim verification. CoRR, abs/1910.02655, 2019. URL http://arxiv.org/abs/1910.02655.
17. Tan, Reuben, Kate Saenko, and Bryan A. Plummer. "Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News." arXiv preprint arXiv:2009.07698 (2020). URL: https://arxiv.org/pdf/2009.07698.pdf
18. Thorne, James and Andreas Vlachos. Automated Fact Checking: Task Formulations, Methods and Future Directions. In Proceedings of the 27th International Conference on Computational Linguistics, pages 3346–3359, jun 2018. URL http://arxiv.org/abs/1806.07687.
19. Vlachos, A., Sebastian Riedel. Fact checking: Task definition and dataset construction. In
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Sci- ence, pages 18–22, Baltimore, MD, USA, June 2014. Association for Computational Linguistics. doi: 10.3115/v1/W14-2508. URL https://www.aclweb.org/anthology/W14-2508.
20. Wang, William Yang. ”liar, liar pants on fire”: A new benchmark dataset for fake news detection. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 422–426, 2017.
21. Yoneda, T., Jeff Mitchell, Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. UCL machine reading group: Four factor framework for fact finding (HexaF). In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 97–102, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5515. URL https://www.aclweb.org/anthology/W18-5515.
22. Zhong, W., Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. CoRR, abs/1909.03745, 2019. URL http://arxiv.org/abs/1909.03745.
23. Umer, M., Z. Imtiaz, S. Ullah, A. Mehmood, G. S. Choi and B. -W. On, "Fake News Stance Detection Using Deep Learning Architecture (CNN-LSTM)," in IEEE Access, vol. 8, pp. 156695-156706, 2020, doi: 10.1109/ACCESS.2020.3019735.
24. Zhou, Z., Huankang Guan, Meghana Bhat, and Justin Hsu. Fake news detection via nlp is vulnerable to adversarial attacks. In Proceedings of the 11th International Conference on Agents and Artificial Intelligence. SCITEPRESS - Science and Technology Publications, 2019. ISBN 9789897583506. doi: 10.5220/0007566307940800. URL http://dx.doi.org/10.5220/ 0007566307940800.

![Image](https://ichef.bbci.co.uk/images/ic/400xn/p088bnqx.jpg)

